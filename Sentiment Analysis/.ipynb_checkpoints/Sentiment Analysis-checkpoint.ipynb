{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "Sentiment analysis is the interpretation and classification of emotions (positive, negative and neutral) within text data using text analysis techniques. Sentiment analysis allows businesses to identify customer sentiment toward products, brands or services in online conversations and feedback.\n",
    "\n",
    "In this project, we will work with IMDB movie reviews and develop different machine learning models to predict a given review as positive or negative.\n",
    "\n",
    "1. We will begin cleaning the reviews(removing stopwords, punctuation and digits). \n",
    "2. We will then use two different approaches to create feature space. \n",
    "3. In the 1st one, we will consider the presence/absence of the word(feature) into account. We will send 2 classification models, the 1st one will be support vector machine and the second one would be a random forest classifier. We will calculate the f1 score, test set accuracy and 10 most ifluential words for each of the models.\n",
    "4. In the 2nd one, we will use TF-IDF vectorization and also include bigrams in the analysis. We will then build naive bayes and random forest classifier. We will again calculate the f1 score, test set accuracy and 10 most ifluential words for each of the models. \n",
    "5. We also use cross-validation and grid search approach to tune the random forest model in the 2nd appraoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk, re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.naive_bayes import GaussianNB,MultinomialNB\n",
    "from sklearn.metrics import accuracy_score,f1_score\n",
    "from sklearn.model_selection import train_test_split,GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from collections import namedtuple\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loading train and test data. train.csv contains reviews and train_labels.csv contains the corresponding labels. \n",
    "train=pd.read_table(\"train.csv\",sep=\"\\n\")\n",
    "y_train=pd.read_csv(\"train_labels.csv\")\n",
    "y_test=pd.read_csv(\"test_labels.csv\")\n",
    "train_reviews=list(train[\"review\"])\n",
    "test=pd.read_table(\"test.csv\",sep=\"\\n\")\n",
    "test_reviews=list(test[\"review\"])\n",
    "all_reviews=train_reviews+test_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "1000\n",
      "(4000, 1)\n",
      "(1000, 1)\n",
      "For a movie that gets no respect there sure are a lot of memorable quotes listed for this gem. Imagine a movie where Joe Piscopo is actually funny! Maureen Stapleton is a scene stealer. The Moroni character is an absolute scream. Watch for Alan \"The Skipper\" Hale jr. as a police Sgt.\n",
      "Based on an actual story, John Boorman shows the struggle of an American doctor, whose husband and son were murdered and she was continually plagued with her loss. A holiday to Burma with her sister seemed like a good idea to get away from it all, but when her passport was stolen in Rangoon, she could not leave the country with her sister, and was forced to stay back until she could get I.D. papers from the American embassy. To fill in a day before she could fly out, she took a trip into the countryside with a tour guide. \"I tried finding something in those stone statues, but nothing stirred in me. I was stone myself.\" <br /><br />Suddenly all hell broke loose and she was caught in a political revolt. Just when it looked like she had escaped and safely boarded a train, she saw her tour guide get beaten and shot. In a split second she decided to jump from the moving train and try to rescue him, with no thought of herself. Continually her life was in danger. <br /><br />Here is a woman who demonstrated spontaneous, selfless charity, risking her life to save another. Patricia Arquette is beautiful, and not just to look at; she has a beautiful heart. This is an unforgettable story. <br /><br />\"We are taught that suffering is the one promise that life always keeps.\"\n"
     ]
    }
   ],
   "source": [
    "#finding size of train and test data\n",
    "print(len(train_reviews))\n",
    "print(len(test_reviews))\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)\n",
    "print((train_reviews[0]))\n",
    "print((test_reviews[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the purpose of clean_review function is to remove stopwords, punctuation, \n",
    "#special characters as well as extra spaces\n",
    "def clean_review(review):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    review = review.lower()\n",
    "    # Clean the text\n",
    "    review = re.sub(r\"<br />\", \" \", review)\n",
    "    review = re.sub(r\"[^a-z]\", \" \", review)\n",
    "    review = re.sub(r\"   \", \" \", review) # Remove any extra spaces\n",
    "    review = re.sub(r\"  \", \" \", review)\n",
    "    #remove stopwords\n",
    "    tokenized = word_tokenize(review)\n",
    "    review = [w for w in tokenized if not w in stop_words]\n",
    "    review = \" \".join(review)\n",
    "\n",
    "\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning every review present in train_reviews and test_reviews using the function defined above. \n",
    "\n",
    "It should return two lists, one each for cleaned train and test reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_data(train_reviews,test_reviews) :\n",
    "    '''Input - train and test reviews\n",
    "    Output - cleaned train and test reviews'''\n",
    "    \n",
    "    \n",
    "    new_train=[]\n",
    "    new_test=[]\n",
    "    for x in train_reviews:\n",
    "        new_train.append(clean_review(x))\n",
    "        \n",
    "    for y in test_reviews:\n",
    "        new_test.append(clean_review(y))\n",
    "\n",
    "    \n",
    "    return new_train, new_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000\n",
      "1000\n",
      "movie gets respect sure lot memorable quotes listed gem imagine movie joe piscopo actually funny maureen stapleton scene stealer moroni character absolute scream watch alan skipper hale jr police sgt\n",
      "\n",
      "\n",
      "based actual story john boorman shows struggle american doctor whose husband son murdered continually plagued loss holiday burma sister seemed like good idea get away passport stolen rangoon could leave country sister forced stay back could get papers american embassy fill day could fly took trip countryside tour guide tried finding something stone statues nothing stirred stone suddenly hell broke loose caught political revolt looked like escaped safely boarded train saw tour guide get beaten shot split second decided jump moving train try rescue thought continually life danger woman demonstrated spontaneous selfless charity risking life save another patricia arquette beautiful look beautiful heart unforgettable story taught suffering one promise life always keeps\n"
     ]
    }
   ],
   "source": [
    "train_reviews,test_reviews=clean_data(train_reviews,test_reviews)\n",
    "\n",
    "print(len(train_reviews))\n",
    "print(len(test_reviews))\n",
    "print(train_reviews[0])\n",
    "print(\"\\n\")\n",
    "print(test_reviews[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bag of words using sklearn's CountVectorizer. We will create features as the presence/absence of a word in a particular review. Hence, keep the binary parameter in CountVectorizer to True.\n",
    "We first train the vectorizer on train reviews to create a feature space and then transform the test reviews as well.\n",
    "\n",
    "We then return the trained vectorizer object and train and test features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "def create_bag_words(train_reviews,test_reviews) :\n",
    "    '''Input - train and test reviews\n",
    "    Output - the trained vectorizer and train and test feature matrix'''\n",
    "    \n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    train = vectorizer.fit_transform(train_reviews)\n",
    "    train_features=train.toarray()\n",
    "    test=vectorizer.transform(test_reviews)\n",
    "    test_features=test.toarray()\n",
    "    \n",
    "    return vectorizer,train_features,test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 34444)\n",
      "(1000, 34444)\n"
     ]
    }
   ],
   "source": [
    "vectorizer,X_train,X_test=create_bag_words(train_reviews,test_reviews)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a support vector machine classifier using Scikit learn. We also return classification accuracy on test data and 5 most important features(words) each for positive and negative sentiment using a linear kernel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import f1_score\n",
    "def execute_svm_model(X_train,y_train,X_test,y_test) :\n",
    "    '''Input - train and test features and labels\n",
    "    Output - trained svm model, test_accuracy, f1_score and important words'''\n",
    "    \n",
    "    results_svm=[]\n",
    "    new_trainy=y_train.to_numpy()\n",
    "    new_y_train=[x for z in new_trainy for x in z ]\n",
    "    new_testy=y_test.to_numpy()\n",
    "    new_y_test=[x for z in new_testy for x in z ]\n",
    "    clf = SVC(kernel='linear')\n",
    "    clf.fit(X_train, new_y_train)\n",
    "    predictionsvm=clf.predict(X_test)\n",
    "    yy=clf.score(X_test,new_y_test)\n",
    "    lol=clf.coef_\n",
    "    lol2=[]\n",
    "    for x in lol:\n",
    "        lol2.extend(x) \n",
    "    names=vectorizer.get_feature_names()\n",
    "    Z = [x for _,x in sorted(zip(lol2,names),reverse=True)]\n",
    "    total=Z[:5]+Z[-5:]\n",
    "    results_svm.append(clf)\n",
    "    results_svm.append(yy)\n",
    "    results_svm.append(f1_score(new_y_test, predictionsvm, average='binary'))\n",
    "    results_svm.append(total)\n",
    "    \n",
    "    return results_svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.839\n",
      "0.8398009950248755\n",
      "['worst', 'waste', 'terrible', 'awful', 'poor', 'excellent', 'best', 'history', 'amazing', 'wonderful']\n"
     ]
    }
   ],
   "source": [
    "results_svm=execute_svm_model(X_train,y_train,X_test,y_test)\n",
    "print(results_svm[1])\n",
    "print(results_svm[2])\n",
    "print(results_svm[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a random forest classifier using Scikit learn. It also returns classification accuracy on test data and 5 most important features(words) each for positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "def execute_randomForest_model(X_train,y_train,X_test,y_test) :\n",
    "    \n",
    "    '''Input - train and test features and labels\n",
    "    Output - trained random forest model, test_accuracy, f1_score and important words'''\n",
    "    \n",
    "    results=[]\n",
    "    clf_2 = RandomForestClassifier(n_estimators=400, random_state=13)\n",
    "    clf_2 = RandomForestClassifier(n_estimators=400, random_state=13)\n",
    "    new_ytrain=y_train.to_numpy()\n",
    "    new_y_train=[x for z in new_ytrain for x in z ]\n",
    "    new_ytest=y_test.to_numpy()\n",
    "    new_y_test=[x for z in new_ytest for x in z ]\n",
    "    clf_2.fit(X_train, new_y_train)\n",
    "    ans=clf_2.predict(X_test)\n",
    "    names=vectorizer.get_feature_names()\n",
    "    tt=accuracy_score(ans, new_y_test)\n",
    "    featres=clf_2.feature_importances_\n",
    "    Z_a = [x for _,x in sorted(zip(featres,names),reverse=True)]\n",
    "    total=Z_a[:10]\n",
    "    \n",
    "    results.append(clf_2)\n",
    "    results.append(tt)\n",
    "    results.append(f1_score(new_y_test,ans, average='binary'))\n",
    "    results.append(total)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.851\n",
      "0.8490374873353597\n",
      "['bad', 'worst', 'great', 'waste', 'terrible', 'awful', 'best', 'boring', 'nothing', 'minutes']\n"
     ]
    }
   ],
   "source": [
    "results_rf1=execute_randomForest_model(X_train,y_train,X_test,y_test)\n",
    "print(results_rf1[1])\n",
    "print(results_rf1[2])\n",
    "print(results_rf1[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating bigrams given a text review and return the list of bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "def generate_bigrams(review) :\n",
    "    '''Input - a review\n",
    "    Output - all possible bigrams for that review'''\n",
    "\n",
    "    \n",
    "    bigrm = list(nltk.bigrams(review.split()))\n",
    "    new_bigrm=[]\n",
    "    for x in bigrm:\n",
    "        new_bigrm.append(' '.join(x))\n",
    "    \n",
    "    return new_bigrm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "movie gets respect sure lot memorable quotes listed gem imagine movie joe piscopo actually funny maureen stapleton scene stealer moroni character absolute scream watch alan skipper hale jr police sgt\n",
      "['movie gets', 'gets respect', 'respect sure', 'sure lot', 'lot memorable', 'memorable quotes', 'quotes listed', 'listed gem', 'gem imagine', 'imagine movie', 'movie joe', 'joe piscopo', 'piscopo actually', 'actually funny', 'funny maureen', 'maureen stapleton', 'stapleton scene', 'scene stealer', 'stealer moroni', 'moroni character', 'character absolute', 'absolute scream', 'scream watch', 'watch alan', 'alan skipper', 'skipper hale', 'hale jr', 'jr police', 'police sgt']\n"
     ]
    }
   ],
   "source": [
    "print(train_reviews[0])\n",
    "print(generate_bigrams(train_reviews[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a bag of words using sklearn's TfidfVectorizer. We will create features using tf-idf values of words. Also, we will now include bigrams inaddition to unigrams(words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def create_bag_words_2(train_reviews,test_reviews) :\n",
    "    '''Input - train and test reviews\n",
    "    Output - the trained vectorizer and train and test feature matrix'''\n",
    "    \n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1, 2))\n",
    "    vectorizer.fit(train_reviews)\n",
    "    X_train2 = vectorizer.transform(train_reviews)\n",
    "    X_test2 = vectorizer.transform(test_reviews)\n",
    "    \n",
    "    return vectorizer,X_train2,X_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4000, 399070)\n",
      "(1000, 399070)\n"
     ]
    }
   ],
   "source": [
    "vectorizer2,X_train2,X_test2=create_bag_words_2(train_reviews,test_reviews)\n",
    "print(X_train2.shape)\n",
    "print(X_test2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a naive bayes classifier using Scikit learn. It also returns classification accuracy on test data and 5 most important features(words) each for positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "def execute_naive_bayes(X_train,y_train,X_test,y_test) :\n",
    "    '''Input - train and test features and labels\n",
    "    Output - trained naive bayes model, test_accuracy, f1_score and important words'''\n",
    "    \n",
    "    results_nb=[]\n",
    "    new_ytrain=y_train.to_numpy()\n",
    "    new_y_train=[x for z in new_ytrain for x in z ]\n",
    "    new_ytest=y_test.to_numpy()\n",
    "    new_y_test=[x for z in new_ytest for x in z ]\n",
    "\n",
    "    clf_NB = MultinomialNB()\n",
    "    clf_NB.fit(X_train2, new_y_train)\n",
    "    pred_NB=clf_NB.predict(X_test2)\n",
    "    tt=accuracy_score(pred_NB, new_y_test)\n",
    "    class_z=clf_NB.feature_log_prob_[0,:]\n",
    "    class_o=clf_NB.feature_log_prob_[1,:]\n",
    "    new_names= vectorizer2.get_feature_names()\n",
    "    Z_NB_class_one = [x for _,x in sorted(zip(class_o,new_names),reverse=True)]\n",
    "    Z_NB_class_two = [x for _,x in sorted(zip(class_z,new_names),reverse=True)]\n",
    "    total=Z_NB_class_two[:5]+Z_NB_class_one[:5]\n",
    "    results_nb.append(clf_NB)\n",
    "    results_nb.append(tt)\n",
    "    results_nb.append(f1_score(new_y_test,pred_NB, average='binary'))\n",
    "    results_nb.append(total)\n",
    "    \n",
    "    \n",
    "    return results_nb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.844\n",
      "0.8517110266159696\n",
      "['movie', 'film', 'one', 'great', 'like', 'movie', 'film', 'one', 'bad', 'like']\n"
     ]
    }
   ],
   "source": [
    "results_nb=execute_naive_bayes(X_train2,y_train,X_test2,y_test)\n",
    "print(results_nb[1])\n",
    "print(results_nb[2])\n",
    "print(results_nb[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a random forest classifier using Scikit learn. It also returns classification accuracy on test data and 5 most important features(words) each for positive and negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def execute_randomForest_model2(X_train,y_train,X_test,y_test) :\n",
    "    '''Input - train and test features and labels\n",
    "    Output - trained random forest model, test_accuracy, f1_score and important words'''\n",
    "   \n",
    "    results_rf2=[]\n",
    "    new_ytrain=y_train.to_numpy()\n",
    "    new_y_train=[x for z in new_ytrain for x in z ]\n",
    "    new_ytest=y_test.to_numpy()\n",
    "    new_y_test=[x for z in new_ytest for x in z ]\n",
    "    \n",
    "    clf_rf2 = RandomForestClassifier(n_estimators=400, random_state=13)\n",
    "    clf_rf2.fit(X_train,new_y_train)\n",
    "    \n",
    "    pred2=clf_rf2.predict(X_test)\n",
    "    tt=accuracy_score(pred2, new_y_test)\n",
    "    \n",
    "    new_names=vectorizer2.get_feature_names()\n",
    "    rf2_feat=clf_rf2.feature_importances_\n",
    "    \n",
    "    Z_rf2 = [x for _,x in sorted(zip(rf2_feat,new_names),reverse=True)]\n",
    "    \n",
    "    results_rf2.append(clf_rf2)\n",
    "    results_rf2.append(tt)\n",
    "    results_rf2.append(f1_score(new_y_test, pred2, average='binary'))\n",
    "    total=Z_rf2[:10]\n",
    "    results_rf2.append(total)\n",
    "    \n",
    "    return results_rf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.837\n",
      "0.8454976303317535\n",
      "['bad', 'worst', 'waste', 'great', 'minutes', 'best', 'nothing', 'awful', 'terrible', 'even']\n"
     ]
    }
   ],
   "source": [
    "results_rf2=execute_randomForest_model2(X_train2,y_train,X_test2,y_test)\n",
    "print(results_rf2[1])\n",
    "print(results_rf2[2])\n",
    "print(results_rf2[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning the max_depth parameter of random forest model created above to best performance. We will use GridSeachCV present in Scikit learn library to get 3-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def tune_random_forest(rf_model,cv,parameters,X_train,y_train) :\n",
    "    '''\n",
    "    Input - random forest model, cross validation, parameters and data\n",
    "    Output - fitted GridSeachCV model\n",
    "    '''\n",
    "    \n",
    "    new_ytrain=y_train.to_numpy()\n",
    "    new_y_train=[x for z in new_ytrain for x in z ]\n",
    "    clf_grid= GridSearchCV(rf_model, parameters)\n",
    "    \n",
    "    clf_grid.fit(X_train,new_y_train)\n",
    "    \n",
    "    \n",
    "    return clf_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters={'max_depth' : [i for i in range(4,12)]}\n",
    "tuned_rf=tune_random_forest(results_rf2[0],3,parameters,X_train2,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83625\n",
      "{'max_depth': 11}\n"
     ]
    }
   ],
   "source": [
    "print(tuned_rf.best_score_)\n",
    "print(tuned_rf.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now visualize results obtained from all the 4 models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_results() :\n",
    "    '''\n",
    "    output - results dataframe\n",
    "    '''\n",
    "    \n",
    "    test_accuracies=[]\n",
    "    f1_scores=[]\n",
    "    imp_words=[]\n",
    "    headers=['svm','rf1','nb','rf2']\n",
    "    test_accuracies.append(results_svm[1])\n",
    "    test_accuracies.append(results_rf1[1])\n",
    "    test_accuracies.append(results_nb[1])\n",
    "    test_accuracies.append(results_rf2[1]) \n",
    "    f1_scores.append(results_svm[2])\n",
    "    f1_scores.append(results_rf1[2])\n",
    "    f1_scores.append(results_nb[2])\n",
    "    f1_scores.append(results_rf2[2]) \n",
    "    imp_words.append(results_svm[3])\n",
    "    imp_words.append(results_rf1[3])\n",
    "    imp_words.append(results_nb[3])\n",
    "    imp_words.append(results_rf2[3]) \n",
    "                     \n",
    "    table = [test_accuracies,f1_scores,imp_words]\n",
    "    df = pd.DataFrame(table,columns=headers).transpose()              \n",
    "    df.columns = ['test_accuracies', 'f1_scores','imp_words']\n",
    "#     df.rows = ['svm','rf1','nb','rf2']\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>test_accuracies</th>\n",
       "      <th>f1_scores</th>\n",
       "      <th>imp_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>svm</td>\n",
       "      <td>0.839</td>\n",
       "      <td>0.839801</td>\n",
       "      <td>[worst, waste, terrible, awful, poor, excellen...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rf1</td>\n",
       "      <td>0.851</td>\n",
       "      <td>0.849037</td>\n",
       "      <td>[bad, worst, great, waste, terrible, awful, be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>nb</td>\n",
       "      <td>0.844</td>\n",
       "      <td>0.851711</td>\n",
       "      <td>[movie, film, one, great, like, movie, film, o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>rf2</td>\n",
       "      <td>0.837</td>\n",
       "      <td>0.845498</td>\n",
       "      <td>[bad, worst, waste, great, minutes, best, noth...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    test_accuracies f1_scores  \\\n",
       "svm           0.839  0.839801   \n",
       "rf1           0.851  0.849037   \n",
       "nb            0.844  0.851711   \n",
       "rf2           0.837  0.845498   \n",
       "\n",
       "                                             imp_words  \n",
       "svm  [worst, waste, terrible, awful, poor, excellen...  \n",
       "rf1  [bad, worst, great, waste, terrible, awful, be...  \n",
       "nb   [movie, film, one, great, like, movie, film, o...  \n",
       "rf2  [bad, worst, waste, great, minutes, best, noth...  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df=visualize_results()\n",
    "results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
