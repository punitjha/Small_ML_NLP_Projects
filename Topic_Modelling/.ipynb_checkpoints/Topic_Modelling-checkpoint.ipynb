{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic Modelling\n",
    "\n",
    "In this project, we will work with research papers published on different aspects of coronaviruses over the years. Our goal is to use topic modelling to know different areas each research paper talks about and answer some important questions regarding the viruses.\n",
    "\n",
    "1. We will begin by first extracting full body text, abstract and title from each paper and cleaning them.\n",
    "2. We will then use gensim library to create a LDA topic model on the extracted body texts.\n",
    "3. We will then use topic modelling and try to find most relevant papers on aspects like vaccine and respiratory viruses.\n",
    "4. Finally, we will look at coherence score as a measure of tuning the number of topics in LDA topic model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from  sklearn.cluster import AgglomerativeClustering,SpectralClustering,KMeans\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
    "import seaborn as sns\n",
    "import scispacy\n",
    "import spacy\n",
    "from gensim.models.ldamodel import LdaModel,CoherenceModel\n",
    "from gensim import corpora\n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting stopwords and lemmatizer\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "customize_stop_words = set([\n",
    "    'doi', 'preprint', 'copyright', 'org', 'https', 'et', 'al', 'author', 'figure', 'table',\n",
    "    'rights', 'reserved', 'permission', 'use', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', 'al.', 'Elsevier', 'PMC', 'CZI',\n",
    "    '-PRON-', 'usually','study','also'])\n",
    "stop_words=set(list(customize_stop_words)+list(stop_words))\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_abstract(abstract):\n",
    "    '''Clean the text, with the option to remove stopwords'''\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    abstract = abstract.lower()\n",
    "    # Clean the text\n",
    "    abstract = re.sub(r\"<br />\", \" \", abstract)\n",
    "    abstract = re.sub(r\"[^a-z]\", \" \", abstract)\n",
    "    abstract = re.sub(r\"   \", \" \", abstract) # Remove any extra spaces\n",
    "    abstract = re.sub(r\"  \", \" \", abstract)\n",
    "    #remove stopwords\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    tokenized = word_tokenize(abstract)\n",
    "    abstract = [lemmatizer.lemmatize(w) for w in tokenized if not w in stop_words and len(w) > 3]\n",
    "    #abstract = \" \".join(abstract)\n",
    "\n",
    "\n",
    "    \n",
    "    # Return a list of words\n",
    "    return abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below code cell prepares the following important objects for analysis :\n",
    "\n",
    "1. cleaned_text - list of lists where each sublist is cleaned full text of a research paper. \n",
    "\n",
    "2. text - list of lists where each sublist is full text of a research paper.\n",
    "\n",
    "3. cleaned_titles - list of lists where each sublist is cleaned title of a research paper. \n",
    "\n",
    "4. titles - list of lists where each sublist is title of a research paper. \n",
    "\n",
    "5. abstracts - list of lists where each sublist is abstract of a research paper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracting full text, abstracts and titles and corresponding paper ids from json data.\n",
    "# we will clean the full text and titles.\n",
    "cleaned_text=[]\n",
    "cleaned_titles=[]\n",
    "paper_ids=[]\n",
    "text=[]\n",
    "abstracts=[]\n",
    "titles=[]\n",
    "count=0\n",
    "for file in os.listdir(\"pdf_json\") :\n",
    "    with open('pdf_json/' + file) as json_data:\n",
    "        data=json.load(json_data)\n",
    "        l=data['body_text']\n",
    "        l1=data['abstract']\n",
    "        if len(l1)==0 or len(l)==0:\n",
    "            continue\n",
    "        count+=1\n",
    "        abstract=\"\"\n",
    "        paper_ids.append(data['paper_id'])\n",
    "        for d in l :\n",
    "            abstract+=d[\"text\"]+\" \"\n",
    "        if 'coronavirus' in abstract :\n",
    "            text.append(abstract)\n",
    "            abstract=clean_abstract(abstract)\n",
    "            cleaned_text.append(abstract)\n",
    "            abstract=\"\"\n",
    "            for d in l1 :\n",
    "                abstract+=d[\"text\"]+\" \"\n",
    "            abstracts.append(abstract)\n",
    "            titles.append(data['metadata']['title'])\n",
    "            cleaned_titles.append(clean_abstract(data['metadata']['title']))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating dictionary and corpus objects which will be used for creating gensim topic model. We use the corpora package of the gensim library. The input to the function is the cleaned_text list which we have created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.corpora.dictionary import Dictionary\n",
    "def create_corpus(text) :\n",
    "    \n",
    "    dictio = Dictionary(text)\n",
    "    corpus = [dictio.doc2bow(texts) for texts in text]\n",
    "    \n",
    "    return dictio, corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aberrant\n",
      "able\n",
      "absent\n",
      "abundant\n",
      "accepted\n",
      "according\n",
      "acid\n",
      "acknowledgment\n",
      "acquire\n",
      "acquired\n",
      "act\n",
      "activate\n",
      "activated\n",
      "activates\n",
      "activating\n",
      "activation\n",
      "actively\n",
      "activity\n",
      "adaptive\n",
      "adaptor\n"
     ]
    }
   ],
   "source": [
    "dictionary,corpus=create_corpus(cleaned_text)\n",
    "for i in range(20) :\n",
    "    print(dictionary[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating lda topic model using gensim. Inputs will be dictionary and corpus object created above and the no. of top important words from each topic we want to extract.\n",
    "\n",
    "While creating the model, we  keep no. of topics to 8 and random_state=25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lda_model(dictionary,corpus,n_words) :\n",
    "    \n",
    "    lda = LdaModel(corpus, num_topics=8,random_state=25,id2word=dictionary)\n",
    "    \n",
    "    return lda,lda.show_topics(num_topics=8, num_words=n_words, formatted=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "(0, '0.015*\"virus\" + 0.008*\"infection\" + 0.008*\"sample\" + 0.006*\"case\" + 0.006*\"disease\" + 0.006*\"respiratory\" + 0.006*\"viral\" + 0.005*\"patient\" + 0.005*\"cell\" + 0.004*\"time\" + 0.004*\"positive\" + 0.004*\"child\" + 0.004*\"human\" + 0.004*\"result\" + 0.004*\"clinical\" + 0.004*\"group\" + 0.004*\"pathogen\" + 0.003*\"detection\" + 0.003*\"data\" + 0.003*\"influenza\" + 0.003*\"study\" + 0.003*\"detected\" + 0.003*\"reported\" + 0.003*\"analysis\" + 0.003*\"animal\" + 0.003*\"strain\" + 0.003*\"however\" + 0.003*\"control\" + 0.003*\"year\" + 0.003*\"assay\" + 0.003*\"different\" + 0.003*\"high\" + 0.003*\"number\" + 0.003*\"outbreak\" + 0.002*\"found\" + 0.002*\"infected\" + 0.002*\"rate\" + 0.002*\"well\" + 0.002*\"protein\" + 0.002*\"associated\"')\n"
     ]
    }
   ],
   "source": [
    "lda_model,topics=create_lda_model(dictionary,corpus,40)\n",
    "print(len(topics))\n",
    "print(topics[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, '0.016*\"patient\" + 0.009*\"infection\" + 0.006*\"respiratory\" + 0.006*\"case\" + 0.006*\"virus\" + 0.005*\"data\" + 0.004*\"group\" + 0.004*\"influenza\" + 0.004*\"disease\" + 0.004*\"viral\" + 0.004*\"level\" + 0.003*\"analysis\" + 0.003*\"result\" + 0.003*\"cell\" + 0.003*\"clinical\" + 0.003*\"hospital\" + 0.003*\"year\" + 0.003*\"sample\" + 0.003*\"study\" + 0.003*\"number\" + 0.003*\"sars\" + 0.003*\"risk\" + 0.003*\"treatment\" + 0.003*\"control\" + 0.003*\"pneumonia\" + 0.003*\"child\" + 0.003*\"however\" + 0.003*\"among\" + 0.003*\"model\" + 0.003*\"associated\" + 0.003*\"rate\" + 0.003*\"health\" + 0.003*\"time\" + 0.003*\"day\" + 0.003*\"significant\" + 0.002*\"population\" + 0.002*\"test\" + 0.002*\"effect\" + 0.002*\"protein\" + 0.002*\"symptom\"')\n",
      "(2, '0.024*\"cell\" + 0.014*\"infection\" + 0.012*\"virus\" + 0.009*\"protein\" + 0.009*\"mouse\" + 0.008*\"viral\" + 0.005*\"expression\" + 0.005*\"infected\" + 0.005*\"response\" + 0.004*\"gene\" + 0.004*\"replication\" + 0.004*\"type\" + 0.003*\"host\" + 0.003*\"level\" + 0.003*\"result\" + 0.003*\"human\" + 0.003*\"effect\" + 0.003*\"shown\" + 0.003*\"immune\" + 0.003*\"disease\" + 0.003*\"induced\" + 0.003*\"control\" + 0.003*\"patient\" + 0.003*\"anti\" + 0.002*\"activity\" + 0.002*\"activation\" + 0.002*\"pathway\" + 0.002*\"antiviral\" + 0.002*\"sars\" + 0.002*\"specific\" + 0.002*\"data\" + 0.002*\"observed\" + 0.002*\"factor\" + 0.002*\"cytokine\" + 0.002*\"antibody\" + 0.002*\"study\" + 0.002*\"role\" + 0.002*\"well\" + 0.002*\"compared\" + 0.002*\"however\"')\n",
      "(3, '0.011*\"virus\" + 0.010*\"viral\" + 0.009*\"sequence\" + 0.008*\"protein\" + 0.007*\"cell\" + 0.006*\"infection\" + 0.004*\"sample\" + 0.004*\"analysis\" + 0.004*\"gene\" + 0.004*\"genome\" + 0.004*\"strain\" + 0.004*\"host\" + 0.003*\"human\" + 0.003*\"group\" + 0.003*\"activity\" + 0.003*\"site\" + 0.003*\"data\" + 0.003*\"result\" + 0.003*\"binding\" + 0.003*\"read\" + 0.003*\"replication\" + 0.003*\"region\" + 0.003*\"identified\" + 0.003*\"specie\" + 0.002*\"disease\" + 0.002*\"could\" + 0.002*\"influenza\" + 0.002*\"however\" + 0.002*\"found\" + 0.002*\"type\" + 0.002*\"high\" + 0.002*\"number\" + 0.002*\"acid\" + 0.002*\"three\" + 0.002*\"structure\" + 0.002*\"different\" + 0.002*\"based\" + 0.002*\"hcov\" + 0.002*\"well\" + 0.002*\"effect\"')\n",
      "(4, '0.024*\"cell\" + 0.014*\"protein\" + 0.013*\"virus\" + 0.007*\"viral\" + 0.006*\"infection\" + 0.004*\"infected\" + 0.004*\"gene\" + 0.004*\"expression\" + 0.004*\"membrane\" + 0.004*\"control\" + 0.004*\"result\" + 0.003*\"assay\" + 0.003*\"human\" + 0.003*\"time\" + 0.003*\"replication\" + 0.003*\"shown\" + 0.003*\"analysis\" + 0.003*\"host\" + 0.003*\"activity\" + 0.003*\"different\" + 0.003*\"observed\" + 0.003*\"effect\" + 0.003*\"interaction\" + 0.003*\"level\" + 0.003*\"sequence\" + 0.003*\"antibody\" + 0.003*\"well\" + 0.002*\"data\" + 0.002*\"anti\" + 0.002*\"ifitm\" + 0.002*\"specific\" + 0.002*\"binding\" + 0.002*\"mouse\" + 0.002*\"fusion\" + 0.002*\"response\" + 0.002*\"mers\" + 0.002*\"three\" + 0.002*\"however\" + 0.002*\"sample\" + 0.002*\"structure\"')\n",
      "(5, '0.022*\"cell\" + 0.010*\"protein\" + 0.010*\"virus\" + 0.008*\"infection\" + 0.006*\"gene\" + 0.006*\"antibody\" + 0.005*\"viral\" + 0.004*\"sample\" + 0.004*\"expression\" + 0.004*\"human\" + 0.004*\"infected\" + 0.003*\"sequence\" + 0.003*\"response\" + 0.003*\"level\" + 0.003*\"well\" + 0.003*\"activity\" + 0.003*\"mouse\" + 0.003*\"group\" + 0.003*\"specific\" + 0.003*\"binding\" + 0.003*\"however\" + 0.003*\"anti\" + 0.003*\"different\" + 0.003*\"analysis\" + 0.003*\"serum\" + 0.003*\"result\" + 0.003*\"data\" + 0.003*\"could\" + 0.003*\"control\" + 0.003*\"assay\" + 0.003*\"mers\" + 0.002*\"receptor\" + 0.002*\"performed\" + 0.002*\"immune\" + 0.002*\"antigen\" + 0.002*\"peptide\" + 0.002*\"pedv\" + 0.002*\"disease\" + 0.002*\"type\" + 0.002*\"study\"')\n",
      "(6, '0.010*\"protein\" + 0.008*\"virus\" + 0.008*\"cell\" + 0.007*\"viral\" + 0.004*\"human\" + 0.004*\"infection\" + 0.004*\"host\" + 0.004*\"genome\" + 0.004*\"data\" + 0.004*\"sample\" + 0.004*\"structure\" + 0.004*\"sequence\" + 0.003*\"analysis\" + 0.003*\"activity\" + 0.003*\"gene\" + 0.003*\"disease\" + 0.003*\"pathogen\" + 0.003*\"membrane\" + 0.003*\"sars\" + 0.002*\"study\" + 0.002*\"based\" + 0.002*\"interaction\" + 0.002*\"binding\" + 0.002*\"specie\" + 0.002*\"domain\" + 0.002*\"model\" + 0.002*\"high\" + 0.002*\"number\" + 0.002*\"time\" + 0.002*\"result\" + 0.002*\"health\" + 0.002*\"level\" + 0.002*\"however\" + 0.002*\"specific\" + 0.002*\"site\" + 0.002*\"replication\" + 0.002*\"three\" + 0.002*\"different\" + 0.002*\"region\" + 0.002*\"well\"')\n",
      "(7, '0.018*\"virus\" + 0.010*\"cell\" + 0.008*\"protein\" + 0.007*\"viral\" + 0.007*\"sequence\" + 0.007*\"vaccine\" + 0.006*\"infection\" + 0.006*\"human\" + 0.006*\"strain\" + 0.005*\"gene\" + 0.004*\"response\" + 0.004*\"infected\" + 0.004*\"genome\" + 0.004*\"sars\" + 0.004*\"antibody\" + 0.003*\"group\" + 0.003*\"mers\" + 0.003*\"pedv\" + 0.003*\"type\" + 0.003*\"host\" + 0.003*\"different\" + 0.003*\"sample\" + 0.003*\"animal\" + 0.003*\"based\" + 0.003*\"result\" + 0.003*\"analysis\" + 0.003*\"mouse\" + 0.003*\"specie\" + 0.002*\"disease\" + 0.002*\"found\" + 0.002*\"however\" + 0.002*\"number\" + 0.002*\"acid\" + 0.002*\"time\" + 0.002*\"study\" + 0.002*\"site\" + 0.002*\"high\" + 0.002*\"influenza\" + 0.002*\"immune\" + 0.002*\"compared\"')\n"
     ]
    }
   ],
   "source": [
    "#Printing the list of topics to see which one has the highest proportion of certains words\n",
    "for x in range(1,8):\n",
    "    print(topics[x])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the top 20 papers for a given topic number.  A given paper belongs to the topic no. whose proportion is the highest among all topics in the paper. Input is the topic no.(0 based indexing). After getting all the papers belonging to the topic no. k(input), we sort them based on the proportion of topic k they have in descending order and return 20 papers with highest amount of topic k in them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_articles(k) :\n",
    "    \n",
    "    doc_topics=lda_model.get_document_topics(corpus)\n",
    "    track_dict=[]\n",
    "    for x,y in enumerate(doc_topics):\n",
    "        for tup in y:\n",
    "            if (tup[0]==k):\n",
    "                track_dict.append((x,tup[1]))\n",
    "    sort_flat=sorted(track_dict, key = lambda x: x[1],reverse=True)\n",
    "    return sort_flat[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2595, 0.99980026),\n",
       " (2409, 0.9997993),\n",
       " (2086, 0.9997805),\n",
       " (2785, 0.99975777),\n",
       " (1382, 0.99968994),\n",
       " (2331, 0.9996309),\n",
       " (3608, 0.9995981),\n",
       " (3659, 0.9995814),\n",
       " (1805, 0.99954945),\n",
       " (2431, 0.9995453),\n",
       " (271, 0.9995075),\n",
       " (1106, 0.9995037),\n",
       " (2699, 0.99944925),\n",
       " (2406, 0.9994471),\n",
       " (2468, 0.99942887),\n",
       " (1879, 0.99942493),\n",
       " (2117, 0.9994037),\n",
       " (2857, 0.99937415),\n",
       " (2271, 0.9993598),\n",
       " (212, 0.99933577)]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_20_0=get_top_articles(0)\n",
    "top_20_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we know about vaccine development efforts for viruses?\n",
    "To answer we get the most relevant 20 articles on the subject of vaccines for various viruses.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_articles=get_top_articles(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recombinant Chimeric Transmissible Gastroenteritis Virus (TGEV)-Porcine Epidemic Diarrhea Virus (PEDV) Virus Provides Protection against Virulent PEDV\n",
      "\n",
      "\n",
      "Trypsin-independent porcine epidemic diarrhea virus US strain with altered virus entry mechanism\n",
      "\n",
      "\n",
      "The Program for New Century Excellent Talents in University of Ministry of Education of P.R. China (NCET-10-0144), Sponsored by Chang Jiang Scholar Candidates Programme for Provincial Universities in Heilongjiang\n",
      "\n",
      "\n",
      "Contribution of porcine aminopeptidase N to porcine deltacoronavirus infection\n",
      "\n",
      "\n",
      "Genetic manipulation of porcine deltacoronavirus reveals insights into NS6 and NS7 functions: a novel strategy for vaccine design\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,x in vaccine_articles[:5] :\n",
    "    print(titles[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the output of function 2 we see that among the 8 topics listed the probability of word vaccine occurring in them for the top 40 words is low. However,   topic[4] has the word vaccine with a probablity =0.003*\"vaccine\". So we choose it to get the most relevant 20 articles on the subject of vaccines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do we know about other viruses causing respiratory problems in adults and children?\n",
    "\n",
    "We get the most relevant 20 articles on the subject of respiratory problems in adults and children."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp_articles=get_top_articles(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Respiratory Virus Infections in Hematopoietic Cell Transplant Recipients\n",
      "\n",
      "\n",
      "viruses Perspective Potential Maternal and Infant Outcomes from Coronavirus 2019-nCoV (SARS-CoV-2) Infecting Pregnant Women: Lessons from SARS, MERS, and Other Human Coronavirus Infections\n",
      "\n",
      "\n",
      "Exposure Patterns Driving Ebola Transmission in West Africa: A Retrospective Observational Study International Ebola Response Team\n",
      "\n",
      "\n",
      "Bacterial and viral pathogen spectra of acute respiratory infections in under-5 children in hospital settings in Dhaka city\n",
      "\n",
      "\n",
      "Goal-Oriented Respiratory Management for Critically Ill Patients with Acute Respiratory Distress Syndrome\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i,x in resp_articles[:5] :\n",
    "    print(titles[i])\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing the output of function 2, we see that among the 8 topics listed the probability of word \"resipiratory\" and \"child\" occurring in them for the top 40 words is highest in topic[7]. So we choose topic[7] to get the most relevant 20 articles on the subject of respiratory problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate coherence score of LDA model for different no. of topics. Input will be a list of no. of topics for which we want to calculate coherence score. Output will be a list of tuples where the 1st element is no. of topics and 2nd element is coherence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "def get_coherence_scores(n_topics) :\n",
    "    \n",
    "    cm_score=[]\n",
    "    for x in n_topics:\n",
    "        model2 = LdaModel(corpus, num_topics=x,random_state=25,id2word=dictionary)\n",
    "        cm = CoherenceModel(model=model2,texts=cleaned_text, corpus=corpus,dictionary=dictionary, coherence='c_v')\n",
    "        cm_score.append((x,cm.get_coherence()))\n",
    "    \n",
    "    return cm_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(3, 0.3237877027385651),\n",
       " (6, 0.34323254567915923),\n",
       " (8, 0.3383538853673067),\n",
       " (10, 0.3341392450816718)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_topics=[3,6,8,10]\n",
    "coherence_scores=get_coherence_scores(n_topics)\n",
    "coherence_scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
